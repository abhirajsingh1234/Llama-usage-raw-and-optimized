{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add2f7a2-edc3-4b76-9d88-e36427d8576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab45ce79-57d4-41a9-a241-d048f1717542",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token=\"your access token\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Load embedding model (use a lightweight one like 'all-MiniLM-L6-v2')\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24bcdb29-d7da-4375-bb3a-a3f6f1a62275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retrieval(question,context):\n",
    "    \n",
    "    # Load embedding model (use a lightweight one like 'all-MiniLM-L6-v2')\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    sentences = re.findall(r'[^.]+\\.?', context)  # Capture sentences including their full stop\n",
    "    # Context divided into list\n",
    "    context = [sentence.strip() for sentence in sentences if sentence]\n",
    "    \n",
    "    # Encode question and context sentences\n",
    "    question_embedding = embedding_model.encode(question, convert_to_tensor=True)\n",
    "    context_embeddings = embedding_model.encode(context, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity between question and each sentence\n",
    "    similarities = util.pytorch_cos_sim(question_embedding, context_embeddings)\n",
    "    \n",
    "    # Find the most relevant sentence (highest similarity score)\n",
    "    best_match_idx = torch.argmax(similarities).item()\n",
    "    \n",
    "    relevant_sentence = context[best_match_idx]\n",
    "\n",
    "    return relevant_sentence\n",
    "    \n",
    "    \n",
    "def truncate_at_last_full_stop(text):\n",
    "    last_dot_index = text.rfind(\".\")  # Find the last full stop\n",
    "    if last_dot_index != -1:  \n",
    "        return text[:last_dot_index + 1]  # Keep text up to the last full stop\n",
    "    return text\n",
    "\n",
    "def question_splitter(question):\n",
    "    pattern = r'\\b(?:and|or|also|as well as|besides|in addition to|moreover|furthermore|plus|along with|together with|what about|how about|while|whereas|likewise|similarly|on the other hand|not to mention|coupled with|additionally|then|next|after that|before that|simultaneously|just like|compared to|contrary to|beside that)\\b'\n",
    "\n",
    "    # Find all connectors in the question\n",
    "    connectors = re.findall(pattern, question, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Split the question into parts using connectors\n",
    "    split_sentences = [q.strip() for q in re.split(pattern, question, flags=re.IGNORECASE) if q.strip()]\n",
    "    \n",
    "    return split_sentences, connectors  # Return list of sentences + list of connectors used\n",
    "\n",
    "# Test Cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97a7a8bb-6f25-4926-b9ff-bdf817b96ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Eiffel Tower is located in Paris, France and  Construction of Eiffel Tower was completed in 1889 and  Eiffel tower stands 330 meters tall\n"
     ]
    }
   ],
   "source": [
    "\n",
    "whole_context = \"The Eiffel Tower is located in Paris, France. Construction of eiffel tower was completed in 1889. Eiffel tower stands 330 meters tall.\"\n",
    "# question = \"Where is the Eiffel Tower located?\"\n",
    "# question = \"how tall is Eiffel Tower?\"\n",
    "question = \"where is eiffel tower located and when was Eiffel Tower constructed and how tall it is?\"\n",
    "answer_format = \"Target:\"\n",
    "answer_list = []\n",
    "actual_answer=''\n",
    "\n",
    "\n",
    "question_list,connectors = question_splitter(question)\n",
    "for q in question_list:\n",
    "    relevant_context=context_retrieval(q,whole_context)\n",
    "    input_text = f\"Input: {relevant_context}\\n{q}\\n{answer_format}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\",truncation=True, padding=True)\n",
    "\n",
    "    output = model.generate(**inputs, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,num_beams=5)\n",
    "    answer=tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    final_answer=truncate_at_last_full_stop(answer.replace(input_text,''))\n",
    "    answer_list.append(final_answer)\n",
    "    \n",
    "for idx,ans in enumerate(answer_list):\n",
    "    try:\n",
    "        if len(answer_list) >1:\n",
    "            if  idx<len(answer_list)-1:\n",
    "                actual_answer= actual_answer + f\"{ans} {connectors[idx]} \"\n",
    "            else:\n",
    "                actual_answer= actual_answer + f\"{ans}\"\n",
    "        else:\n",
    "            actual_answer= actual_answer + f\"{ans}\"\n",
    "    except:\n",
    "        pass\n",
    "actual_answer= actual_answer.replace('.','')\n",
    "print(actual_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b0924-dbb3-4a00-91b8-4a6d1b4d2f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c661e6-2030-4851-b8e9-bdaf83de941f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8506d5-493e-479e-8182-322253cd2ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
