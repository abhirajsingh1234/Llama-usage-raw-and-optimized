{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aef93d-a6da-4bfc-ae80-f1df6944a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Deep Explanation of the Code Flow**   first read this theory for code explaination then go to code present in second cell\n",
    "\n",
    "This code implements a **Retrieval-Augmented Generation (RAG) pipeline** using **LangChain, ChromaDB, a local LLaMA model, and Tavily web search API**. The purpose of this pipeline is to **retrieve, validate, and generate answers** based on existing documents and external search results while ensuring factual correctness.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setting Up the Environment**\n",
    "- The code starts by installing necessary libraries, which include **LangChain components, ChromaDB (vector database), FireCrawl (web crawling), Tavily (web search API), and a local LLaMA model for inference**.\n",
    "- Environment variables are set for LangChain tracing and API keys for authentication with LangChain and Tavily.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Document Ingestion & Processing**\n",
    "- The system is designed to **scrape data from predefined URLs** using `FireCrawlLoader`, which fetches web content.\n",
    "- The retrieved data is then **split into smaller text chunks** using `RecursiveCharacterTextSplitter`. This step is critical because long documents may not fit into the model‚Äôs context window.\n",
    "- The code then **filters complex metadata**, keeping only relevant and simple metadata fields (e.g., text, numbers, and boolean values).\n",
    "- These processed document chunks are **stored in ChromaDB**, where they are converted into embedding vectors using `GPT4AllEmbeddings` for fast retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Retrieving Documents for a User Query**\n",
    "- When a user asks a question, the system first attempts to **retrieve relevant documents from ChromaDB**.\n",
    "- The retrieved document is then **graded using a retrieval grader model** (`Chatollama` LLM), which determines whether the document is relevant.\n",
    "  - If **no relevant document is found**, the system **performs a web search** using Tavily API to fetch external information.\n",
    "  - If a document is found but **marked as irrelevant**, it is discarded, and web search is used as a fallback.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Generating an Answer Using the Retrieved Context**\n",
    "- The system uses **RAG (Retrieval-Augmented Generation)** by feeding the retrieved document and user query into a **prompt template**.\n",
    "- The local LLaMA model (`Chatollama`) generates a response using the retrieved document‚Äôs content.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Verifying for Hallucination**\n",
    "- Since LLMs can **hallucinate (generate incorrect or non-factual information)**, the generated answer is checked against the retrieved document.\n",
    "- If **the generated answer contains information that is not found in the retrieved document**, it is marked as **hallucinated**.\n",
    "  - If hallucination is detected, the system **performs another web search** and regenerates a new answer using the newly fetched context.\n",
    "  - If the answer is **not hallucinated**, it is considered **factually valid** and returned to the user.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Final Answer Delivery**\n",
    "- After multiple **validation loops** (retrieval ‚Üí validation ‚Üí web search ‚Üí hallucination check), the final response is returned.\n",
    "- This ensures that the **answer is accurate, factually grounded, and based on a reliable source**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Strengths of This Approach**\n",
    "1. **Combining Static & Dynamic Knowledge:**  \n",
    "   - Uses **pre-indexed documents (ChromaDB)** for efficiency.  \n",
    "   - Uses **real-time web search (Tavily)** to fetch fresh data when needed.\n",
    "\n",
    "2. **Multi-Level Filtering & Validation:**  \n",
    "   - **Retrieval Grader** ensures that irrelevant documents are discarded.  \n",
    "   - **Hallucination Checker** ensures that incorrect answers are corrected.  \n",
    "   - **Web Search Fallback** ensures information is always available.\n",
    "\n",
    "3. **Minimizing LLM Hallucination:**  \n",
    "   - The system does **not allow the model to generate answers freely** without checking if the context supports it.\n",
    "   - If **no valid answer is found, it explicitly states ‚ÄúI don‚Äôt know‚Äù instead of guessing.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Takeaway**\n",
    "This pipeline creates a **reliable, context-aware, and verifiable question-answering system**. It ensures that the **answers are backed by valid sources**, preventing misinformation and hallucination. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188ced9-de4d-4f0d-856e-0dc74ce951b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python pt4all firecrawl-py\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import Chatollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Set environment variables for LangChain tracing and API keys\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"xxxxxxx\"  # Replace with actual API key\n",
    "os.environ['TAVILY_API_KEY'] = \"xxxxx\"  # Replace with actual API key\n",
    "\n",
    "# Define local LLaMA model\n",
    "local_llm = \"llama3\"  # Ensure this is properly loaded in your environment\n",
    "\n",
    "# URLs for scraping to extract information for document storage\n",
    "urls = [\n",
    "    \"https://www.al-jason.com/learning-ai/how-to-reduce-ulm-cost\",\n",
    "    \"https://www.ai-jason.com/learning-ai/gpt5-lim\",\n",
    "    \"https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3\",\n",
    "]\n",
    "\n",
    "# Load documents using FireCrawlLoader from the provided URLs\n",
    "docs = [FireCrawlLoader(api_key=\"xxxxx\", url=url, mode=\"scrape\").load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents into smaller chunks for efficient retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter out complex metadata to keep only relevant data for storage\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Store the processed documents into ChromaDB for retrieval\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define a grading prompt to assess document relevance to user queries\n",
    "llm = Chatollama(model=local_llm, format=\"json\", temperature=0)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a grader assessing the relevance of a retrieved document to a user question. \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\n",
    "    Provide the binary score as JSON with a single key 'score'.\n",
    "    Document: {document}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Define hallucination detection to verify if model-generated answers align with context\n",
    "hallucination_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are checking if the model-generated answer is factually correct based on the retrieved context.\n",
    "    If the answer contains information not found in the context, classify it as hallucinated.\n",
    "    Provide a binary score 'yes' or 'no' as JSON with a single key 'hallucination'.\n",
    "    Context: {context}\n",
    "    Answer: {answer}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"answer\"]\n",
    ")\n",
    "hallucination_checker = hallucination_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Web search function using Tavily API in case no relevant documents are found\n",
    "def web_search(query):\n",
    "    client = TavilyClient()\n",
    "    results = client.search(query=query, num_results=3)\n",
    "    return \"\\n\".join([r['content'] for r in results['results']]) if 'results' in results else \"No additional information found.\"\n",
    "\n",
    "# Define the RAG (Retrieval-Augmented Generation) prompt to answer user queries\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the retrieved context to answer the question. If unsure, say \"I don't know\".\n",
    "    Answer concisely in three sentences maximum.\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "rag_chain = rag_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Function to process user questions and return an accurate response\n",
    "def answer_question(question):\n",
    "    # Retrieve relevant documents from ChromaDB\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[0].page_content if docs else \"\"\n",
    "    \n",
    "    # Check if the retrieved document is relevant; if not, perform a web search\n",
    "    if not docs or retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})['score'] == \"no\":\n",
    "        print(\"No relevant document found in ChromaDB. Performing web search...\")\n",
    "        doc_txt = web_search(question)\n",
    "    \n",
    "    # Generate an answer using the retrieved or searched context\n",
    "    answer = rag_chain.invoke({\"context\": doc_txt, \"question\": question})\n",
    "    \n",
    "    # Verify if the generated answer is hallucinated\n",
    "    if hallucination_checker.invoke({\"context\": doc_txt, \"answer\": answer})['hallucination'] == \"yes\":\n",
    "        print(\"Detected hallucination. Fetching more reliable sources...\")\n",
    "        doc_txt = web_search(question)\n",
    "        answer = rag_chain.invoke({\"context\": doc_txt, \"question\": question})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"Where to buy iPhone 5?\"\n",
    "response = answer_question(question)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
