{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97aca3-94fc-42f3-bd34-bc69e4f57a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aef93d-a6da-4bfc-ae80-f1df6944a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the **deep explanation**   first read this theory for code explaination then go to code present in second cell\n",
    "\n",
    "# ---\n",
    "\n",
    "# # **Deep Explanation of the Code Flow**  \n",
    "\n",
    "# This code implements a **Retrieval-Augmented Generation (RAG) pipeline** using **LangChain, ChromaDB, a local LLaMA model, and Tavily web search API**. The goal is to **retrieve, validate, and generate accurate answers** based on stored documents and external search results, while ensuring **factual correctness** and **problem resolution**.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **1. Setting Up the Environment**  \n",
    "# - The code starts by **installing necessary dependencies** for LangChain, vector databases (ChromaDB), web search (Tavily), and the local LLaMA model.  \n",
    "# - **API keys and environment variables** are set to enable LangChain tracing and authentication with Tavily.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **2. Document Ingestion & Processing**  \n",
    "# This step ensures **high-quality knowledge retrieval** by:  \n",
    "# 1. **Scraping documents** from predefined URLs using `FireCrawlLoader`.  \n",
    "# 2. **Splitting documents into chunks** using `RecursiveCharacterTextSplitter` to fit within model context limits.  \n",
    "# 3. **Filtering metadata** to remove unnecessary complexity.  \n",
    "# 4. **Storing processed documents** in **ChromaDB** as embedding vectors, enabling fast and efficient retrieval.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **3. Retrieving Documents for a User Query**  \n",
    "# When a user submits a question, the system follows these steps:  \n",
    "# 1. **ChromaDB is queried** for relevant documents.  \n",
    "# 2. The retrieved document is sent to a **retrieval grader** (`Chatollama`) that decides:  \n",
    "#    - If **relevant**, the document is used for answering.  \n",
    "#    - If **not relevant**, a **web search using Tavily** is triggered.  \n",
    "\n",
    "# This ensures that **only high-quality context** is used for generating answers.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **4. Generating an Answer Using the Retrieved Context**  \n",
    "# Once a relevant document is found:  \n",
    "# 1. The **retrieved document and question** are passed into a **RAG prompt**.  \n",
    "# 2. The local **LLaMA model (`Chatollama`) generates an answer**.  \n",
    "# 3. The generated response is then **checked for hallucination**.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **5. Hallucination Detection**  \n",
    "# Since **LLMs can hallucinate (generate incorrect answers)**, the pipeline includes:  \n",
    "# - A **hallucination checker** that verifies whether the generated answer **stays true to the retrieved context**.  \n",
    "# - If hallucination is detected:  \n",
    "#   1. The system **performs another web search**.  \n",
    "#   2. The model **regenerates the answer** using **newly retrieved information**.  \n",
    "\n",
    "# This ensures the final answer is **factually accurate** and prevents misinformation.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **6. Final Answer Verification (New Step Added)**  \n",
    "# Even after passing the **hallucination check**, the generated answer is further evaluated using a **final answer grader**.  \n",
    "# - This step ensures that the generated response actually **resolves the user‚Äôs query**.  \n",
    "# - The answer is graded as:  \n",
    "#   - `\"yes\"` ‚Üí If the answer sufficiently resolves the query.  \n",
    "#   - `\"no\"` ‚Üí If the answer does not fully address the query.  \n",
    "\n",
    "# If the answer is **not useful**, the system **performs another web search and regenerates the response**.  \n",
    "\n",
    "# This final step guarantees that the model provides a **clear, direct, and actionable response** rather than vague or unhelpful information.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **7. Final Answer Delivery**  \n",
    "# - After multiple **verification loops** (retrieval ‚Üí hallucination check ‚Üí final grading), the **best possible answer** is presented to the user.  \n",
    "# - This process ensures that the response is **accurate, contextually relevant, and directly addresses the question**.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **Core Strengths of This Approach**  \n",
    "# 1. **Combining Static & Dynamic Knowledge:**  \n",
    "#    - Uses **stored documents (ChromaDB)** for efficiency.  \n",
    "#    - Uses **real-time web search (Tavily)** for fresh, up-to-date information.  \n",
    "\n",
    "# 2. **Multi-Level Filtering & Validation:**  \n",
    "#    - **Retrieval Grader:** Ensures **only relevant** documents are used.  \n",
    "#    - **Hallucination Checker:** Prevents **factually incorrect** answers.  \n",
    "#    - **Final Answer Grader:** Ensures that the answer is **actually useful**.  \n",
    "\n",
    "# 3. **Minimizing LLM Hallucination:**  \n",
    "#    - If **no valid answer is found**, the system explicitly states **‚ÄúI don‚Äôt know‚Äù** instead of generating misleading information.  \n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Final Takeaway**  \n",
    "# This pipeline ensures **high-quality, verifiable, and context-aware responses** by combining **retrieval-based and generative AI techniques**. With **multi-step validation**, the system prevents incorrect answers, irrelevant responses, and hallucination‚Äîcreating a **robust and reliable AI assistant**. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188ced9-de4d-4f0d-856e-0dc74ce951b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python pt4all firecrawl-py\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import Chatollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Set environment variables for LangChain tracing and API keys\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"xxxxxxx\"  # Replace with actual API key\n",
    "os.environ['TAVILY_API_KEY'] = \"xxxxx\"  # Replace with actual API key\n",
    "\n",
    "# Define local LLaMA model\n",
    "local_llm = \"llama3\"  # Ensure this is properly loaded in your environment\n",
    "\n",
    "# URLs for scraping to extract information for document storage\n",
    "urls = [\n",
    "    \"https://www.al-jason.com/learning-ai/how-to-reduce-ulm-cost\",\n",
    "    \"https://www.ai-jason.com/learning-ai/gpt5-lim\",\n",
    "    \"https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3\",\n",
    "]\n",
    "\n",
    "# Load documents using FireCrawlLoader from the provided URLs\n",
    "docs = [FireCrawlLoader(api_key=\"xxxxx\", url=url, mode=\"scrape\").load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents into smaller chunks for efficient retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter out complex metadata to keep only relevant data for storage\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Store the processed documents into ChromaDB for retrieval\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define a grading prompt to assess document relevance to user queries\n",
    "llm = Chatollama(model=local_llm, format=\"json\", temperature=0)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a grader assessing the relevance of a retrieved document to a user question. \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\n",
    "    Provide the binary score as JSON with a single key 'score'.\n",
    "    Document: {document}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# Define the RAG (Retrieval-Augmented Generation) prompt to answer user queries\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the retrieved context to answer the question. If unsure, say \"I don't know\".\n",
    "    Answer concisely in three sentences maximum.\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "rag_chain = rag_prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# Define hallucination detection to verify if model-generated answers align with context\n",
    "hallucination_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are checking if the model-generated answer is factually correct based on the retrieved context.\n",
    "    If the answer contains information not found in the context, classify it as hallucinated.\n",
    "    Provide a binary score 'yes' or 'no' as JSON with a single key 'hallucination'.\n",
    "    Context: {context}\n",
    "    Answer: {answer}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"answer\"]\n",
    ")\n",
    "hallucination_checker = hallucination_prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "# Define a final answer grader to check if the answer resolves the question\n",
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a grader assessing whether an answer is useful to resolve a question. \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful.\n",
    "    Provide the binary score as JSON with a single key 'score'.\n",
    "    Here is the answer: \n",
    "    \n",
    "    {generation}\n",
    "    \n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"]\n",
    ")\n",
    "answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# Web search function using Tavily API in case no relevant documents are found\n",
    "def web_search(query):\n",
    "    client = TavilyClient()\n",
    "    results = client.search(query=query, num_results=3)\n",
    "    return \"\\n\".join([r['content'] for r in results['results']]) if 'results' in results else \"No additional information found.\"\n",
    "    \n",
    "\n",
    "# Function to process user questions and return an accurate response\n",
    "def answer_question(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[0].page_content if docs else \"\"\n",
    "    \n",
    "    if not docs or retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})['score'] == \"no\":\n",
    "        print(\"No relevant document found in ChromaDB. Performing web search...\")\n",
    "        doc_txt = web_search(question)\n",
    "    \n",
    "    answer = rag_chain.invoke({\"context\": doc_txt, \"question\": question})\n",
    "    \n",
    "    if hallucination_checker.invoke({\"context\": doc_txt, \"answer\": answer})['hallucination'] == \"yes\":\n",
    "        print(\"Detected hallucination. Fetching more reliable sources...\")\n",
    "        doc_txt = web_search(question)\n",
    "        answer = rag_chain.invoke({\"context\": doc_txt, \"question\": question})\n",
    "    \n",
    "    if answer_grader.invoke({\"question\": question, \"generation\": answer})['score'] == \"no\":\n",
    "        print(\"Answer did not resolve the question. Performing another web search...\")\n",
    "        doc_txt = web_search(question)\n",
    "        answer = rag_chain.invoke({\"context\": doc_txt, \"question\": question})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"Where to buy iPhone 5?\"\n",
    "response = answer_question(question)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
